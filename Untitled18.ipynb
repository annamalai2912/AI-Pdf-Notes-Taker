{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQHl7qOr09Osv4dvR78rrb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamalai2912/AI-Pdf-Notes-Taker/blob/master/Untitled18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "owZudwVPU7nP",
        "outputId": "156d24a4-a510-4eb4-e827-58af4e1104a4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.178.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.99.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "ü§ñ AI SQL Generator for Google Colab\n",
            "Choose an option:\n",
            "1. Interactive Mode\n",
            "2. Run Demo\n",
            "üöÄ AI-Powered SQL Generator\n",
            "==================================================\n",
            "Enter your prompt and I'll generate a complete SQL file with sample data!\n",
            "\n",
            "Examples:\n",
            "‚Ä¢ 'Create a SQL file with 100 student records for Sathyabama college with roll numbers starting from 195114217'\n",
            "‚Ä¢ 'Generate employee database with 50 records for TechCorp company'\n",
            "‚Ä¢ 'Create customer table with 200 entries for e-commerce site'\n",
            "‚Ä¢ 'Make product inventory database with 150 items'\n",
            "==================================================\n",
            "ü§ñ Loading local AI model for SQL generation...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Local AI model loaded successfully!\n",
            "\n",
            "üîÑ Generating SQL file...\n",
            "üéØ Processing prompt: CREATE A sql databsae for  compnay name TechKNots with workshop names(IoT,Robotics,Gen AI,Opencv,Bct),student deatils like name,contact,roll num,ddepartment and paym,ent stays  whther paid or not i need about 15 records for each workshop types\n",
            "üìã Extracted info: {'table_name': 'data_table', 'num_records': 15, 'columns': [], 'data_types': {}, 'constraints': [], 'context': 'CREATE A sql databsae for  compnay name TechKNots with workshop names(IoT,Robotics,Gen AI,Opencv,Bct),student deatils like name,contact,roll num,ddepartment and paym,ent stays  whther paid or not i need about 15 records for each workshop types', 'start_value': None}\n",
            "üèóÔ∏è Generated schema for: data_table\n",
            "‚ùå Error: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
            "Please try with a different prompt or check your input.\n",
            "\n",
            "üîÑ Generating SQL file...\n",
            "üéØ Processing prompt: Table name: TechKNots_Workshops Columns:  - workshop_name TEXT - student_name TEXT - contact TEXT - roll_number TEXT - department TEXT - payment_status TEXT  Instructions: - Workshops include: IoT, Robotics, Gen AI, OpenCV, BCT - Insert 15 student records for each workshop type automatically - payment_status should indicate whether the student has paid (\"Yes\" or \"No\") - Only output valid SQLite CREATE TABLE SQL, no explanations - Use SQLite-compatible syntax\n",
            "üìã Extracted info: {'table_name': 'name', 'num_records': 100, 'columns': [], 'data_types': {}, 'constraints': [], 'context': 'Table name: TechKNots_Workshops Columns:  - workshop_name TEXT - student_name TEXT - contact TEXT - roll_number TEXT - department TEXT - payment_status TEXT  Instructions: - Workshops include: IoT, Robotics, Gen AI, OpenCV, BCT - Insert 15 student records for each workshop type automatically - payment_status should indicate whether the student has paid (\"Yes\" or \"No\") - Only output valid SQLite CREATE TABLE SQL, no explanations - Use SQLite-compatible syntax', 'start_value': None}\n",
            "üèóÔ∏è Generated schema for: name\n",
            "‚ùå Error: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
            "Please try with a different prompt or check your input.\n",
            "\n",
            "üîÑ Generating SQL file...\n",
            "üéØ Processing prompt: Create a SQL file with 15 student records for TechKNots with roll numbers starting from 195114217\n",
            "üìã Extracted info: {'table_name': 'data_table', 'num_records': 100, 'columns': [], 'data_types': {}, 'constraints': [], 'context': 'Create a SQL file with 15 student records for TechKNots with roll numbers starting from 195114217', 'start_value': 195114217}\n",
            "üèóÔ∏è Generated schema for: data_table\n",
            "\n",
            "‚úÖ SQL file created successfully!\n",
            "üìÅ Filename: data_table_20250816_110247.sql\n",
            "üìä Table: data_table\n",
            "üìà Records: Generated as requested\n",
            "üíæ File size: 24691 characters\n",
            "\n",
            "üìã Preview (first 15 lines):\n",
            "----------------------------------------\n",
            " 1: -- AI Generated SQL File\n",
            " 2: -- Created: 2025-08-16 11:02:47\n",
            " 3: -- Table: data_table\n",
            " 4: -- Records: 100\n",
            " 5: \n",
            " 6: -- Create Database\n",
            " 7: CREATE DATABASE IF NOT EXISTS ai_generated_db;\n",
            " 8: USE ai_generated_db;\n",
            " 9: \n",
            "10: -- Drop table if exists\n",
            "11: DROP TABLE IF EXISTS data_table;\n",
            "12: \n",
            "13: -- Create Table\n",
            "14: CREATE TABLE data_table (\n",
            "15:     si_no INT PRIMARY KEY AUTO_INCREMENT,\n",
            "... (truncated)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_1a1c806c-c8db-4e6f-a180-d83dbd895db6\", \"data_table_20250816_110247.sql\", 24691)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ File downloaded successfully!\n",
            "\n",
            "üîç You can now use this SQL file in any database!\n",
            "Sample connection commands:\n",
            "MySQL: mysql -u username -p database_name < data_table_20250816_110247.sql\n",
            "PostgreSQL: psql -U username -d database_name -f data_table_20250816_110247.sql\n",
            "\n",
            "üí≠ Enter your prompt (or 'quit' to exit): quit\n",
            "üëã Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered SQL Generator for Google Colab\n",
        "# This notebook creates SQL files based on natural language prompts\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALLATION CELL - Run this first\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers torch datasets accelerate\n",
        "!pip install google-generativeai  # For Gemini API (optional)\n",
        "!pip install openai  # For OpenAI API (optional)\n",
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================================\n",
        "# AI SQL GENERATOR CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class AIPromptToSQLGenerator:\n",
        "    def __init__(self, use_local_model=True):\n",
        "        \"\"\"\n",
        "        Initialize the AI SQL Generator\n",
        "        Args:\n",
        "            use_local_model: If True, uses local HuggingFace model. If False, uses API (requires key)\n",
        "        \"\"\"\n",
        "        self.use_local_model = use_local_model\n",
        "\n",
        "        if use_local_model:\n",
        "            print(\"ü§ñ Loading local AI model for SQL generation...\")\n",
        "            try:\n",
        "                # Using a coding-focused model for better SQL generation\n",
        "                model_name = \"microsoft/CodeGPT-small-py\"\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "                # Add padding token\n",
        "                if self.tokenizer.pad_token is None:\n",
        "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "                self.generator = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=self.model,\n",
        "                    tokenizer=self.tokenizer,\n",
        "                    max_length=200,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "                print(\"‚úÖ Local AI model loaded successfully!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading model: {e}\")\n",
        "                print(\"üìù Falling back to template-based generation...\")\n",
        "                self.generator = None\n",
        "        else:\n",
        "            print(\"üåê Using API-based generation (requires API key)\")\n",
        "            self.generator = None\n",
        "\n",
        "    def parse_user_prompt(self, prompt):\n",
        "        \"\"\"Parse user prompt to extract requirements\"\"\"\n",
        "        prompt_lower = prompt.lower()\n",
        "\n",
        "        # Extract key information from prompt\n",
        "        extracted_info = {\n",
        "            'table_name': 'data_table',\n",
        "            'num_records': 100,\n",
        "            'columns': [],\n",
        "            'data_types': {},\n",
        "            'constraints': [],\n",
        "            'context': prompt\n",
        "        }\n",
        "\n",
        "        # Extract table name\n",
        "        table_patterns = [\n",
        "            r'table\\s+(?:called\\s+)?(?:named\\s+)?(\\w+)',\n",
        "            r'create\\s+(?:a\\s+)?(\\w+)\\s+table',\n",
        "            r'(\\w+)\\s+database',\n",
        "            r'for\\s+(\\w+)\\s+(?:college|university|company|school)'\n",
        "        ]\n",
        "\n",
        "        for pattern in table_patterns:\n",
        "            match = re.search(pattern, prompt_lower)\n",
        "            if match:\n",
        "                extracted_info['table_name'] = match.group(1).replace(' ', '_')\n",
        "                break\n",
        "\n",
        "        # Extract number of records\n",
        "        num_patterns = [\n",
        "            r'(\\d+)\\s+(?:records|rows|entries|students|employees|customers|data)',\n",
        "            r'generate\\s+(\\d+)',\n",
        "            r'create\\s+(\\d+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in num_patterns:\n",
        "            match = re.search(pattern, prompt_lower)\n",
        "            if match:\n",
        "                extracted_info['num_records'] = int(match.group(1))\n",
        "                break\n",
        "\n",
        "        # Extract starting values (like roll numbers)\n",
        "        start_patterns = [\n",
        "            r'start(?:ing)?\\s+(?:from\\s+|with\\s+|at\\s+)?(\\d+)',\n",
        "            r'roll\\s+number\\s+(\\d+)',\n",
        "            r'id\\s+(?:from\\s+)?(\\d+)'\n",
        "        ]\n",
        "\n",
        "        extracted_info['start_value'] = None\n",
        "        for pattern in start_patterns:\n",
        "            match = re.search(pattern, prompt_lower)\n",
        "            if match:\n",
        "                extracted_info['start_value'] = int(match.group(1))\n",
        "                break\n",
        "\n",
        "        return extracted_info\n",
        "\n",
        "    def generate_schema_from_prompt(self, prompt_info):\n",
        "        \"\"\"Generate database schema based on prompt analysis\"\"\"\n",
        "        table_name = prompt_info['table_name']\n",
        "        context = prompt_info['context'].lower()\n",
        "\n",
        "        # Determine domain and generate appropriate schema\n",
        "        if any(word in context for word in ['student', 'college', 'university', 'school']):\n",
        "            return self.generate_student_schema(table_name, prompt_info)\n",
        "        elif any(word in context for word in ['employee', 'staff', 'company', 'office']):\n",
        "            return self.generate_employee_schema(table_name, prompt_info)\n",
        "        elif any(word in context for word in ['customer', 'client', 'sales', 'order']):\n",
        "            return self.generate_customer_schema(table_name, prompt_info)\n",
        "        elif any(word in context for word in ['product', 'inventory', 'item', 'stock']):\n",
        "            return self.generate_product_schema(table_name, prompt_info)\n",
        "        else:\n",
        "            return self.generate_generic_schema(table_name, prompt_info)\n",
        "\n",
        "    def generate_student_schema(self, table_name, info):\n",
        "        \"\"\"Generate student-specific database schema\"\"\"\n",
        "        start_roll = info.get('start_value', 195114217)\n",
        "\n",
        "        schema = {\n",
        "            'table_name': table_name,\n",
        "            'columns': {\n",
        "                'si_no': 'INT PRIMARY KEY AUTO_INCREMENT',\n",
        "                'name': 'VARCHAR(100) NOT NULL',\n",
        "                'roll_number': 'VARCHAR(20) UNIQUE NOT NULL',\n",
        "                'section': 'CHAR(1)',\n",
        "                'department': 'VARCHAR(100)',\n",
        "                'year': 'INT',\n",
        "                'semester': 'INT',\n",
        "                'age': 'INT',\n",
        "                'gender': \"ENUM('Male', 'Female', 'Other')\",\n",
        "                'phone': 'VARCHAR(15)',\n",
        "                'email': 'VARCHAR(100) UNIQUE',\n",
        "                'address': 'TEXT',\n",
        "                'cgpa': 'DECIMAL(3,2)',\n",
        "                'total_fees': 'DECIMAL(10,2)',\n",
        "                'amount_paid': 'DECIMAL(10,2)',\n",
        "                'payment_status': \"ENUM('Paid', 'Pending', 'Partial')\",\n",
        "                'payment_date': 'DATE',\n",
        "                'hostel': 'BOOLEAN DEFAULT FALSE',\n",
        "                'transport': 'BOOLEAN DEFAULT FALSE',\n",
        "                'scholarship': 'BOOLEAN DEFAULT FALSE',\n",
        "                'admission_date': 'DATE',\n",
        "                'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n",
        "            },\n",
        "            'sample_data_generator': self.generate_student_data,\n",
        "            'start_value': start_roll\n",
        "        }\n",
        "        return schema\n",
        "\n",
        "    def generate_employee_schema(self, table_name, info):\n",
        "        \"\"\"Generate employee-specific database schema\"\"\"\n",
        "        schema = {\n",
        "            'table_name': table_name,\n",
        "            'columns': {\n",
        "                'emp_id': 'INT PRIMARY KEY AUTO_INCREMENT',\n",
        "                'name': 'VARCHAR(100) NOT NULL',\n",
        "                'employee_code': 'VARCHAR(20) UNIQUE NOT NULL',\n",
        "                'department': 'VARCHAR(50)',\n",
        "                'designation': 'VARCHAR(50)',\n",
        "                'salary': 'DECIMAL(10,2)',\n",
        "                'join_date': 'DATE',\n",
        "                'phone': 'VARCHAR(15)',\n",
        "                'email': 'VARCHAR(100) UNIQUE',\n",
        "                'address': 'TEXT',\n",
        "                'manager_id': 'INT',\n",
        "                'status': \"ENUM('Active', 'Inactive', 'On Leave')\",\n",
        "                'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n",
        "            },\n",
        "            'sample_data_generator': self.generate_employee_data,\n",
        "            'start_value': info.get('start_value', 1001)\n",
        "        }\n",
        "        return schema\n",
        "\n",
        "    def generate_customer_schema(self, table_name, info):\n",
        "        \"\"\"Generate customer-specific database schema\"\"\"\n",
        "        schema = {\n",
        "            'table_name': table_name,\n",
        "            'columns': {\n",
        "                'customer_id': 'INT PRIMARY KEY AUTO_INCREMENT',\n",
        "                'name': 'VARCHAR(100) NOT NULL',\n",
        "                'phone': 'VARCHAR(15)',\n",
        "                'email': 'VARCHAR(100) UNIQUE',\n",
        "                'address': 'TEXT',\n",
        "                'city': 'VARCHAR(50)',\n",
        "                'state': 'VARCHAR(50)',\n",
        "                'pincode': 'VARCHAR(10)',\n",
        "                'registration_date': 'DATE',\n",
        "                'last_purchase_date': 'DATE',\n",
        "                'total_purchases': 'DECIMAL(10,2)',\n",
        "                'loyalty_points': 'INT DEFAULT 0',\n",
        "                'status': \"ENUM('Active', 'Inactive', 'VIP')\",\n",
        "                'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n",
        "            },\n",
        "            'sample_data_generator': self.generate_customer_data,\n",
        "            'start_value': info.get('start_value', 1)\n",
        "        }\n",
        "        return schema\n",
        "\n",
        "    def generate_product_schema(self, table_name, info):\n",
        "        \"\"\"Generate product-specific database schema\"\"\"\n",
        "        schema = {\n",
        "            'table_name': table_name,\n",
        "            'columns': {\n",
        "                'product_id': 'INT PRIMARY KEY AUTO_INCREMENT',\n",
        "                'name': 'VARCHAR(100) NOT NULL',\n",
        "                'sku': 'VARCHAR(50) UNIQUE NOT NULL',\n",
        "                'category': 'VARCHAR(50)',\n",
        "                'price': 'DECIMAL(10,2)',\n",
        "                'cost_price': 'DECIMAL(10,2)',\n",
        "                'stock_quantity': 'INT',\n",
        "                'min_stock_level': 'INT',\n",
        "                'supplier': 'VARCHAR(100)',\n",
        "                'description': 'TEXT',\n",
        "                'weight': 'DECIMAL(5,2)',\n",
        "                'dimensions': 'VARCHAR(50)',\n",
        "                'status': \"ENUM('Active', 'Discontinued', 'Out of Stock')\",\n",
        "                'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n",
        "            },\n",
        "            'sample_data_generator': self.generate_product_data,\n",
        "            'start_value': info.get('start_value', 1)\n",
        "        }\n",
        "        return schema\n",
        "\n",
        "    def generate_generic_schema(self, table_name, info):\n",
        "        \"\"\"Generate generic database schema\"\"\"\n",
        "        schema = {\n",
        "            'table_name': table_name,\n",
        "            'columns': {\n",
        "                'id': 'INT PRIMARY KEY AUTO_INCREMENT',\n",
        "                'name': 'VARCHAR(100) NOT NULL',\n",
        "                'description': 'TEXT',\n",
        "                'value': 'VARCHAR(100)',\n",
        "                'status': \"ENUM('Active', 'Inactive')\",\n",
        "                'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP',\n",
        "                'updated_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP'\n",
        "            },\n",
        "            'sample_data_generator': self.generate_generic_data,\n",
        "            'start_value': info.get('start_value', 1)\n",
        "        }\n",
        "        return schema\n",
        "\n",
        "    def generate_student_data(self, record_num, start_value=195114217):\n",
        "        \"\"\"Generate sample student data\"\"\"\n",
        "        first_names = ['Aadhya', 'Aarav', 'Abhinav', 'Aditi', 'Aisha', 'Akash', 'Ananya', 'Arjun', 'Bhavana', 'Chetan', 'Deepika', 'Dev', 'Divya', 'Gaurav', 'Harini', 'Ishaan', 'Jiya', 'Karthik', 'Lakshmi', 'Manoj', 'Nandini', 'Omkar', 'Priya', 'Rahul', 'Sanjana', 'Tanvi', 'Usha', 'Varun', 'Yash', 'Zara']\n",
        "        last_names = ['Sharma', 'Patel', 'Kumar', 'Singh', 'Gupta', 'Reddy', 'Iyer', 'Nair', 'Agarwal', 'Jain', 'Verma', 'Rao', 'Pillai', 'Menon', 'Shah', 'Bhatt']\n",
        "        departments = ['Computer Science Engineering', 'Information Technology', 'Electronics Engineering', 'Mechanical Engineering', 'Civil Engineering', 'Biotechnology']\n",
        "        sections = ['A', 'B', 'C', 'D', 'E']\n",
        "        cities = ['Chennai', 'Bangalore', 'Hyderabad', 'Mumbai', 'Delhi', 'Kolkata', 'Pune', 'Kochi']\n",
        "\n",
        "        name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
        "        roll_number = str(start_value + record_num - 1)\n",
        "        department = random.choice(departments)\n",
        "\n",
        "        return {\n",
        "            'si_no': record_num,\n",
        "            'name': name,\n",
        "            'roll_number': roll_number,\n",
        "            'section': random.choice(sections),\n",
        "            'department': department,\n",
        "            'year': random.randint(1, 4),\n",
        "            'semester': random.randint(1, 8),\n",
        "            'age': random.randint(18, 22),\n",
        "            'gender': random.choice(['Male', 'Female']),\n",
        "            'phone': f\"+91{random.randint(7000000000, 9999999999)}\",\n",
        "            'email': f\"{name.lower().replace(' ', '.')}@college.edu\",\n",
        "            'address': f\"{random.randint(1, 999)} Street, {random.choice(cities)}\",\n",
        "            'cgpa': round(random.uniform(6.5, 9.8), 2),\n",
        "            'total_fees': random.randint(150000, 200000),\n",
        "            'amount_paid': random.randint(100000, 200000),\n",
        "            'payment_status': random.choice(['Paid', 'Pending', 'Partial']),\n",
        "            'payment_date': (datetime.now() - timedelta(days=random.randint(1, 90))).strftime('%Y-%m-%d') if random.choice([True, False]) else None,\n",
        "            'hostel': random.choice([True, False]),\n",
        "            'transport': random.choice([True, False]),\n",
        "            'scholarship': random.choice([True, False]),\n",
        "            'admission_date': (datetime.now() - timedelta(days=random.randint(365, 1460))).strftime('%Y-%m-%d')\n",
        "        }\n",
        "\n",
        "    def generate_employee_data(self, record_num, start_value=1001):\n",
        "        \"\"\"Generate sample employee data\"\"\"\n",
        "        first_names = ['Amit', 'Priya', 'Rajesh', 'Sunita', 'Vikram', 'Meera', 'Arjun', 'Kavitha', 'Suresh', 'Deepika']\n",
        "        last_names = ['Kumar', 'Sharma', 'Patel', 'Singh', 'Gupta', 'Reddy', 'Iyer', 'Shah']\n",
        "        departments = ['IT', 'HR', 'Finance', 'Marketing', 'Operations', 'Sales']\n",
        "        designations = ['Manager', 'Senior Developer', 'Analyst', 'Executive', 'Associate', 'Lead']\n",
        "\n",
        "        name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
        "\n",
        "        return {\n",
        "            'emp_id': record_num,\n",
        "            'name': name,\n",
        "            'employee_code': f\"EMP{start_value + record_num - 1:04d}\",\n",
        "            'department': random.choice(departments),\n",
        "            'designation': random.choice(designations),\n",
        "            'salary': random.randint(30000, 150000),\n",
        "            'join_date': (datetime.now() - timedelta(days=random.randint(30, 3650))).strftime('%Y-%m-%d'),\n",
        "            'phone': f\"+91{random.randint(7000000000, 9999999999)}\",\n",
        "            'email': f\"{name.lower().replace(' ', '.')}@company.com\",\n",
        "            'address': f\"{random.randint(1, 999)} Avenue, Mumbai\",\n",
        "            'manager_id': random.randint(1, 50) if record_num > 10 else None,\n",
        "            'status': random.choice(['Active', 'Inactive', 'On Leave'])\n",
        "        }\n",
        "\n",
        "    def generate_customer_data(self, record_num, start_value=1):\n",
        "        \"\"\"Generate sample customer data\"\"\"\n",
        "        first_names = ['Raj', 'Sita', 'Mohan', 'Gita', 'Ravi', 'Lata', 'Ajay', 'Maya']\n",
        "        last_names = ['Agarwal', 'Bansal', 'Chopra', 'Dwivedi', 'Gupta', 'Jain']\n",
        "        cities = ['Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata', 'Pune', 'Hyderabad']\n",
        "        states = ['Maharashtra', 'Delhi', 'Karnataka', 'Tamil Nadu', 'West Bengal', 'Telangana']\n",
        "\n",
        "        name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
        "        city = random.choice(cities)\n",
        "\n",
        "        return {\n",
        "            'customer_id': record_num,\n",
        "            'name': name,\n",
        "            'phone': f\"+91{random.randint(7000000000, 9999999999)}\",\n",
        "            'email': f\"{name.lower().replace(' ', '.')}@email.com\",\n",
        "            'address': f\"{random.randint(1, 999)} Road, {city}\",\n",
        "            'city': city,\n",
        "            'state': random.choice(states),\n",
        "            'pincode': f\"{random.randint(100000, 999999)}\",\n",
        "            'registration_date': (datetime.now() - timedelta(days=random.randint(1, 730))).strftime('%Y-%m-%d'),\n",
        "            'last_purchase_date': (datetime.now() - timedelta(days=random.randint(1, 180))).strftime('%Y-%m-%d') if random.choice([True, False]) else None,\n",
        "            'total_purchases': round(random.uniform(1000, 50000), 2),\n",
        "            'loyalty_points': random.randint(0, 1000),\n",
        "            'status': random.choice(['Active', 'Inactive', 'VIP'])\n",
        "        }\n",
        "\n",
        "    def generate_product_data(self, record_num, start_value=1):\n",
        "        \"\"\"Generate sample product data\"\"\"\n",
        "        products = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones', 'Tablet', 'Phone', 'Speaker', 'Camera', 'Printer']\n",
        "        categories = ['Electronics', 'Computers', 'Accessories', 'Mobile', 'Audio', 'Photography']\n",
        "        suppliers = ['TechCorp', 'ElectroMax', 'GadgetWorld', 'TechSupply', 'DeviceHub']\n",
        "\n",
        "        product_name = f\"{random.choice(products)} Model {random.randint(100, 999)}\"\n",
        "\n",
        "        return {\n",
        "            'product_id': record_num,\n",
        "            'name': product_name,\n",
        "            'sku': f\"SKU{start_value + record_num - 1:06d}\",\n",
        "            'category': random.choice(categories),\n",
        "            'price': round(random.uniform(500, 50000), 2),\n",
        "            'cost_price': round(random.uniform(300, 30000), 2),\n",
        "            'stock_quantity': random.randint(0, 500),\n",
        "            'min_stock_level': random.randint(5, 50),\n",
        "            'supplier': random.choice(suppliers),\n",
        "            'description': f\"High-quality {product_name.lower()} with advanced features\",\n",
        "            'weight': round(random.uniform(0.1, 5.0), 2),\n",
        "            'dimensions': f\"{random.randint(10, 50)}x{random.randint(10, 50)}x{random.randint(5, 20)} cm\",\n",
        "            'status': random.choice(['Active', 'Discontinued', 'Out of Stock'])\n",
        "        }\n",
        "\n",
        "    def generate_generic_data(self, record_num, start_value=1):\n",
        "        \"\"\"Generate generic sample data\"\"\"\n",
        "        names = ['Item A', 'Item B', 'Item C', 'Product X', 'Service Y', 'Component Z']\n",
        "        descriptions = ['High quality item', 'Premium product', 'Standard service', 'Advanced component']\n",
        "        values = ['Type 1', 'Type 2', 'Category A', 'Category B', 'Level 1', 'Level 2']\n",
        "\n",
        "        return {\n",
        "            'id': record_num,\n",
        "            'name': f\"{random.choice(names)} {record_num}\",\n",
        "            'description': random.choice(descriptions),\n",
        "            'value': random.choice(values),\n",
        "            'status': random.choice(['Active', 'Inactive'])\n",
        "        }\n",
        "\n",
        "    def create_sql_from_prompt(self, user_prompt):\n",
        "        \"\"\"Main function to create SQL from user prompt\"\"\"\n",
        "        print(f\"üéØ Processing prompt: {user_prompt}\")\n",
        "\n",
        "        # Parse the prompt\n",
        "        prompt_info = self.parse_user_prompt(user_prompt)\n",
        "        print(f\"üìã Extracted info: {prompt_info}\")\n",
        "\n",
        "        # Generate schema\n",
        "        schema = self.generate_schema_from_prompt(prompt_info)\n",
        "        print(f\"üèóÔ∏è Generated schema for: {schema['table_name']}\")\n",
        "\n",
        "        # Create SQL content\n",
        "        sql_content = self.build_sql_file(schema, prompt_info['num_records'])\n",
        "\n",
        "        return sql_content, schema\n",
        "\n",
        "    def build_sql_file(self, schema, num_records):\n",
        "        \"\"\"Build complete SQL file\"\"\"\n",
        "        table_name = schema['table_name']\n",
        "        columns = schema['columns']\n",
        "        data_generator = schema['sample_data_generator']\n",
        "        start_value = schema.get('start_value', 1)\n",
        "\n",
        "        # SQL Header\n",
        "        sql_content = f\"\"\"-- AI Generated SQL File\n",
        "-- Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "-- Table: {table_name}\n",
        "-- Records: {num_records}\n",
        "\n",
        "-- Create Database\n",
        "CREATE DATABASE IF NOT EXISTS ai_generated_db;\n",
        "USE ai_generated_db;\n",
        "\n",
        "-- Drop table if exists\n",
        "DROP TABLE IF EXISTS {table_name};\n",
        "\n",
        "-- Create Table\n",
        "CREATE TABLE {table_name} (\n",
        "\"\"\"\n",
        "\n",
        "        # Add columns\n",
        "        column_definitions = []\n",
        "        for col_name, col_type in columns.items():\n",
        "            column_definitions.append(f\"    {col_name} {col_type}\")\n",
        "\n",
        "        sql_content += ',\\n'.join(column_definitions)\n",
        "        sql_content += \"\\n);\\n\\n\"\n",
        "\n",
        "        # Generate and insert data\n",
        "        sql_content += f\"-- Insert Sample Data\\nINSERT INTO {table_name} (\"\n",
        "\n",
        "        # Column names for INSERT\n",
        "        col_names = list(columns.keys())\n",
        "        sql_content += ', '.join(col_names)\n",
        "        sql_content += \") VALUES\\n\"\n",
        "\n",
        "        # Generate data rows\n",
        "        rows = []\n",
        "        for i in range(1, num_records + 1):\n",
        "            record = data_generator(i, start_value)\n",
        "\n",
        "            # Format values for SQL\n",
        "            values = []\n",
        "            for col_name in col_names:\n",
        "                value = record.get(col_name, 'NULL')\n",
        "                if value is None or value == 'NULL':\n",
        "                    values.append('NULL')\n",
        "                elif isinstance(value, str):\n",
        "                    # Escape single quotes in strings\n",
        "                    escaped_value = value.replace(\"'\", \"''\")\n",
        "                    values.append(f\"'{escaped_value}'\")\n",
        "                elif isinstance(value, bool):\n",
        "                    values.append('TRUE' if value else 'FALSE')\n",
        "                else:\n",
        "                    values.append(str(value))\n",
        "\n",
        "            row_sql = f\"({', '.join(values)})\"\n",
        "            rows.append(row_sql)\n",
        "\n",
        "        sql_content += ',\\n'.join(rows)\n",
        "        sql_content += \";\\n\\n\"\n",
        "\n",
        "        # Add useful queries\n",
        "        sql_content += f\"\"\"-- Sample Queries for {table_name}\n",
        "\n",
        "-- 1. View all records\n",
        "SELECT * FROM {table_name} LIMIT 10;\n",
        "\n",
        "-- 2. Count total records\n",
        "SELECT COUNT(*) as total_records FROM {table_name};\n",
        "\n",
        "-- 3. Get records by status (if applicable)\n",
        "\"\"\"\n",
        "\n",
        "        if 'status' in columns:\n",
        "            sql_content += f\"SELECT * FROM {table_name} WHERE status = 'Active';\\n\\n\"\n",
        "        elif 'payment_status' in columns:\n",
        "            sql_content += f\"SELECT * FROM {table_name} WHERE payment_status = 'Paid';\\n\\n\"\n",
        "\n",
        "        # Add group by queries based on table type\n",
        "        if 'department' in columns:\n",
        "            sql_content += f\"-- 4. Group by department\\nSELECT department, COUNT(*) as count FROM {table_name} GROUP BY department;\\n\\n\"\n",
        "\n",
        "        sql_content += \"-- End of generated SQL file\\n\"\n",
        "\n",
        "        return sql_content\n",
        "\n",
        "# ============================================================================\n",
        "# INTERACTIVE INTERFACE FOR GOOGLE COLAB\n",
        "# ============================================================================\n",
        "\n",
        "def main_interactive():\n",
        "    \"\"\"Main interactive function for Google Colab\"\"\"\n",
        "    print(\"üöÄ AI-Powered SQL Generator\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Enter your prompt and I'll generate a complete SQL file with sample data!\")\n",
        "    print(\"\\nExamples:\")\n",
        "    print(\"‚Ä¢ 'Create a SQL file with 100 student records for Sathyabama college with roll numbers starting from 195114217'\")\n",
        "    print(\"‚Ä¢ 'Generate employee database with 50 records for TechCorp company'\")\n",
        "    print(\"‚Ä¢ 'Create customer table with 200 entries for e-commerce site'\")\n",
        "    print(\"‚Ä¢ 'Make product inventory database with 150 items'\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Initialize generator\n",
        "    generator = AIPromptToSQLGenerator(use_local_model=True)\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_prompt = input(\"\\nüí≠ Enter your prompt (or 'quit' to exit): \").strip()\n",
        "\n",
        "        if user_prompt.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not user_prompt:\n",
        "            print(\"‚ö†Ô∏è Please enter a valid prompt!\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Generate SQL\n",
        "            print(\"\\nüîÑ Generating SQL file...\")\n",
        "            sql_content, schema = generator.create_sql_from_prompt(user_prompt)\n",
        "\n",
        "            # Save file\n",
        "            filename = f\"{schema['table_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql\"\n",
        "\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(sql_content)\n",
        "\n",
        "            # Display results\n",
        "            print(f\"\\n‚úÖ SQL file created successfully!\")\n",
        "            print(f\"üìÅ Filename: {filename}\")\n",
        "            print(f\"üìä Table: {schema['table_name']}\")\n",
        "            print(f\"üìà Records: Generated as requested\")\n",
        "            print(f\"üíæ File size: {len(sql_content)} characters\")\n",
        "\n",
        "            # Show preview\n",
        "            lines = sql_content.split('\\n')\n",
        "            print(f\"\\nüìã Preview (first 15 lines):\")\n",
        "            print(\"-\" * 40)\n",
        "            for i, line in enumerate(lines[:15]):\n",
        "                print(f\"{i+1:2d}: {line}\")\n",
        "            if len(lines) > 15:\n",
        "                print(\"... (truncated)\")\n",
        "\n",
        "            # Download option\n",
        "            download = input(f\"\\n‚¨áÔ∏è Download {filename}? (y/n): \").strip().lower()\n",
        "            if download in ['y', 'yes']:\n",
        "                try:\n",
        "                    files.download(filename)\n",
        "                    print(\"‚úÖ File downloaded successfully!\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Download failed: {e}\")\n",
        "\n",
        "            # Show sample query results\n",
        "            print(f\"\\nüîç You can now use this SQL file in any database!\")\n",
        "            print(f\"Sample connection commands:\")\n",
        "            print(f\"MySQL: mysql -u username -p database_name < {filename}\")\n",
        "            print(f\"PostgreSQL: psql -U username -d database_name -f {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            print(\"Please try with a different prompt or check your input.\")\n",
        "\n",
        "# ============================================================================\n",
        "# DEMO FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def run_demo():\n",
        "    \"\"\"Run a demonstration of the SQL generator\"\"\"\n",
        "    print(\"üé¨ Running Demo...\")\n",
        "\n",
        "    generator = AIPromptToSQLGenerator(use_local_model=True)\n",
        "\n",
        "    # Demo prompts\n",
        "    demo_prompts = [\n",
        "        \"Create a SQL file with 10 student records for Sathyabama college with roll numbers starting from 195114217\",\n",
        "        \"Generate employee database with 5 records for TechCorp company\",\n",
        "        \"Create customer table with 8 entries for e-commerce site\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(demo_prompts, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Demo {i}: {prompt}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            sql_content, schema = generator.create_sql_from_prompt(prompt)\n",
        "            filename = f\"demo_{i}_{schema['table_name']}.sql\"\n",
        "\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(sql_content)\n",
        "\n",
        "            print(f\"‚úÖ Created: {filename}\")\n",
        "\n",
        "            # Show a preview\n",
        "            lines = sql_content.split('\\n')\n",
        "            print(\"üìã Preview:\")\n",
        "            for line in lines[10:20]:  # Show middle section\n",
        "                if line.strip():\n",
        "                    print(f\"    {line}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Demo {i} failed: {e}\")\n",
        "\n",
        "    print(f\"\\nüéâ Demo completed! Check the generated files.\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ü§ñ AI SQL Generator for Google Colab\")\n",
        "    print(\"Choose an option:\")\n",
        "    print(\"1. Interactive Mode\")\n",
        "    print(\"2. Run Demo\")\n",
        "\n",
        "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        main_interactive()\n",
        "    elif choice == \"2\":\n",
        "        run_demo()\n",
        "    else:\n",
        "        print(\"Invalid choice. Running interactive mode...\")\n",
        "        main_interactive()"
      ]
    }
  ]
}